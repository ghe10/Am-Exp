Lecture for gv and stirage basic

Grovereeee provides storage services.
      RDS cp deploy oscar engine in ec2 instances.
      End user: has their own vpc, connect to rds cp (random ec2) at some port.
      CSD: client side driver, allowing rds cp to talk to storage.
      
RDS is storage's customer

Storage has its own vpc. You can't jump through vpc boundary to directly access storage without csd.
Operations related to sto is in background.

====================================================================================================

Aurora oscar: mysql, manf : psql

====================================================================================================

volume: think like a file. old mysql use files, we use volume instead.

|---------------------|
|---------------------|

	|--->| 
 kB pages.

We allows rds cp to access any page and get any number of pages in sequence. => read Dage, write log.

Volumes are separate into protection groups (pg).

Default volume is 160 GB, each PG will has 10 GB data (fixed). (Now we might support less than 160 GB for smaller db) so we have 16 pgs for each volume

RDS db instance <==> VOLUME, it is a one to one mapping. 
When we need more data space, we will add new pg to it.

=====================================================================================================

We have i2.8xlarge ec2 instances. has 8 ssds, 800GB each, raw block

these are called sn-demon

we have more than 1000 such nodes in a cluster (across multiple availiability zones).

each sn-demon will manage storage on its machine.

Each pg contains 6 segments on 6 different machines (replication), they are separated amoung three azs. 2 copies in each az.
Then we can tolerate az failure.

=====================================================================================================

Consistency protocol: Quorum, client side driver (csd) need 4/6 for write and 3/6 to read.

During deployment, no more than one segment of each volume is affected.

deployment domain is an aws concept, ?????????

We stop at 34 mins.

======================================================================================================

2018 2 24
tale? segment: for update, not everything
full segment: for data

more like 30GB data + derta

Three segments are T and three are F

so we can have 3/6 full write segment read. ????? really????

=======================================================================================================

storage node

|--------------------|
|--------------------|

	|---| 16 kb page
	 ** derta 1
	 ** derta 2   these things are usually in memory, then go to persisitent storage in disk
	.....

a segemnt in volume, we got a long list of dert.

dert are issued by csd and executed by sn-demon

Read: read the page and apply a lot of update. The dertas are in T segments?????? dert are in inmemory buffer ???

Dertas are in memroy, when go to disk, it is persistent log.

The sn-demons talk to each other to get log sequence number and see if we miss some update log. LSN: log sequence number


recovery: when sn-demon down and restart, it will talk to other sn-demons to fix holes



CPL: consistence point log...

=======================================================================================================

Grover vpc

we have a number of instances (ec2 instances), metadata in ddb. data backuped in S3

we have dependency on swf to execute some things like create volume and so on in backgroud.

We also have cp instances as following:
We also have volume manager(with ebs for log storage), health monitor, workflow, deployment service??, backup manager, restore manager 

Environment set <-> each ec2 instance is tagged by env set
	eg. : VOLUME_MANAGER_SET -> [VOLLUMEN_MANAGER, ..... (some environements)]
	    So we will have /apollo/envSet/env ???


In ddb, we have many tbs. eg, environment table
	we have instance id, ip, environment, health status......... all the related metadata

Next we go through services.
=======================================================================================================
Volumn manager:
    APIS:
	create vol *** most important
	delete vol
	expand volulme
	create net....
	describe volume, pg.....

it will create colume in volumn table in ddb, do workflow reservation (also update ddb). Here we just update ddb!!!!!! SWF will do real work!!!

SWF workflow will watch for changes and read from ddb and really create pgs, segment (16 pg for 160GB default volume, 96 segments 16 * 6) in different zas in ec2 instances

SWF also incharge of delete volumn.

*****
Volumn manager also create anetwork interface to communicate to rds cp (csd actually)


========================================================================================================
2018 03 06
Sextion 2 for grover cp

health monitor: 1. helth of it self 
                2. health of every apollo environment (mapped to the instances, on instances actually) (health, suspect, unhealth), uses ping and timestamp
                3. jvm logs are uploaded for it. HM will then upload it to 1. limber for 1 year storage 2. EMR (elastic map-reduce in aws) to do map-reduce task that join the segments (log staff)

	We have a bugcchaser, it will go to query log when we got a ticket for sth......

HM also cut tickets.

Every environment extends groverEnvironment, so they have the properity of eg. ping HM, upload og...

========================================================================================================
Workflow

trigger based mechienism...
crom based .... ???

eg. process launch workflow        _____\  
    environment starts workflow         / eg env may have workflow to infom that this ec2 instance is too bad and should be terminated. Then a termination workflow (process kind) will terminate it (I think it is somewhere else in system)

eg repair workflow: p2p style talk to peer instances with same pg to fill gap, it is a all-running workflow


We have a bounch of ec2 instances that has our workflow environments, these guys are controlled by AWS SWF. This the flow frame work.

=========================================================================================================

Deployment service (DS) -> it knows what vfi of what env should be on which machine, which software should be deployed to it.

ec2 itself don't have depoyment service originally, so we build one___.

each instance has a deployment agent in each box, they talk to deployment service.

****
There is sth called preferred version set and preferred vfi. (29:00)

envs talks to health monitor

Deploymnet status workflow is there, judge success by the percentage of succeeded nodes

pipeline has an script to upload sth config to S3?????
===========================================================================================================

back manager, restore manager.

For a volume, besides the 6 replications,we also upload it to S3.
it
We can restore from an exact time in a eriod (say 7 days), an new volume will be created in the restore.
Possible cross region volume copy (I think it is through s3).

note volume has base and dertas.

a-chunk?????????????????????????????????????????????? uploaded periodically.

The backup manager will talk to every volume, and gather the dertas exists and upload it to s3


snapshot copy: customer want a copy sometime forever, then the copy id is stored, metadata in dynamodb.



Restore manager:

restore to time, create the volumes, pgs....  a long chain of calling. Also have dchunk. Maybe like data and derta???????????

============================================================================================================

Grover-config is used in many envs




																												

















