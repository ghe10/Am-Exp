https://dev.mysql.com/doc/internals/en/transactions-data-layout.html

-------------------------------------------------------------------
operation encapsulation:
/sql/sql_data_change.h This class encapsulates a data change operation. the related .h and .cc file will lead to actual result we need 
- ha_rows: is a number
- this is an in-memory data structure to store informaion on table to operate on and which column is goign to be updated as some default update
  methods (for example auto increment I guess)

-TABEL is defined in/mysql/table.h

-------------------------------------------------------------------
/sql/sql_insert.h

Query_result_insert
-It is likely to be used for both destination to 1 or more tables
     @param table_list_par   The table reference for the destination table.
     @param table_par        The destination table. May be NULL.
-  this code support insert part and onconflict update part so it has multiple COPY_INFO

Query_result_create
- inherits Query_result_insert
- This is used to create table

Sql_cmd_insert extends Sql_cmd_insert_base

Sql_cmd_insert_select
-------------------------------------------------------------------
main implementation (lowel level impl)
/sql/sql_insert.cc

Query_result_insert::xxxx can be used to find mapping between .h and .cc

Plan: start from 
Sql_cmd_insert::
check what is done there


bool Sql_cmd_insert::execute(THD *thd)

- open_temporary_tables() 

TODO: think about why we do things in this way!!!!

Query_result_insert： 
- need_explain_interceptor () always return true
- prepare() Create the new table from the selected items.
  - create_table_from_items() is invoked to create table, it is going to generate a table with fields only for the selected cols/ or just open existing
- prepare2() 
  - If the result table is the same as one of the source tables (INSERT SELECT),
    the result table is not finally prepared at the join prepair phase.
    Do the final preparation now.
- send_data() *****
  - this write the data to disk? and then return the result of write
  - this is calling write_records() which is also part of the class, faile to understand what iss actually done
  - restore_record() is called in some casee due to changed by on conflict update
  - the return value is whether it fails
- store_values() ***
  - It is filling in the values to table????? not actual write I think
  - Item is a parse tree node
  - fill_record_n_invoke_before_triggers() is invoked, restore_record is done as well. This is in /sql/sql_base.cc. it calls fill_record() to fill in fields with the values
    - fail to understand it as well****
 
 - write_record()
  -  Write a record to table with optional deletion of conflicting records, invoke proper triggers if needed.
  - it is going to write staff to table buffer
  - it is trying to write to tables file one by one (to buffer only I think) error=table->file->ha_write_row(table->record[0])
    - int handler::ha_write_row(uchar *buf)
      - mark transtraction as read_write
      - write data to buffer MYSQL_TABLE_IO_WAIT()
        - MYSQL_TABLE_IO_WAIT(PSI_TABLE_WRITE_ROW, MAX_KEY, error, { error= write_row(buf); }), the write_row() is going to be executed based on case
      - write bin log to its table binlog_log_row() 
      - the write_row(buf) is going to STORAGE_ENGINE!!!! , one example is ha_innodb.cc

/storage/innobase/handler/ha_innodb.cc
ha_innobase::write_row()
- it seems it is using some thread context strategy to find the sql query etc
-  insert graph is build and executed: 
   - mysql_row_templ_t*	templ is build as the graph
      - /* A struct describing a place for an individual column in the MySQL
             row format which is presented to the table handler in ha_innobase.
             This template struct is used to speed up row transformations between
             Innobase and MySQL. */
      - /storage/innobase/include/row0mysql.h
   - innobase_srv_conc_enter_innodb(m_prebuilt);
   - error = row_insert_for_mysql((byte*) record, m_prebuilt);
- afterwords: error handling etc

========================================
INNODB
========================================

  
  InnoDB is a multi-versioned storage engine: it keeps information about old versions of changed rows, to support transactional features such as concurrency and rollback. This information is stored in the tablespace in a data structure called a rollback segment
  
  Each row has last_trans insert/update this row, increasing row id and a pointer to the undo log (rollback segment) for (running trans && used for snapshot read on old staff -> this makes it important to commit trans otherwise we will have a lot of garbage)
  
  In the InnoDB multi-versioning scheme, a row is not physically removed from the database immediately when you delete it with an SQL statement. InnoDB only physically removes the corresponding row and its index records when it discards the update undo log record written for the deletion. This removal operation is called a purge, and it is quite fast, usually taking the same order of time as the SQL statement that did the deletion.
  
  the purge for deleted rows could be lagging if you do insert and delete at same speed,  innodb_max_purge_lag could help to limit it
  
  When a secondary index column is updated, old secondary index records are delete-marked, new records are inserted, and delete-marked records are eventually purged. When a secondary index record is delete-marked or the secondary index page is updated by a newer transaction, InnoDB looks up the database record in the clustered index. In the clustered index, the record's DB_TRX_ID is checked, and the correct version of the record is retrieved from the undo log if the record was modified after the reading transaction was initiated.
  
  Secondary index is updated in a way where new data is directly written while old version is markded as delete and being purged once it is not needed for any read (means trans on it are done)
  
  overing index
An index that includes all the columns retrieved by a query. Instead of using the index values as pointers to find the full table rows, the query returns values from the index structure, saving disk I/O. 


=========================================
INNODB MEMORY STRUCTURE
=========================================

current at https://dev.mysql.com/doc/refman/5.7/en/innodb-buffer-pool.html

Buffer pool
It is using LRU strategy to evict cache.

The pages are saved in to lists, 5/8 are new and remaining are old. The two lists are linked.  The old list is used as canaidate for evict.
If the old page is accessed, it will be moved to top of new list (if access required by user). Tail is going to be evicted at some point I think.

- Linear fatch: fatch based on: if you read pages linearly, following pages will be read as well,  innodb_read_ahead_threshold is going to control how many sequential read is the linke
- Random: this doesn't require sequential read, innodb is going to decide what is going to happen if some sequenal pages are in the buffer pool.

Change Buffer
This is the buffer that used to cache changes to secondary indexes when the index's page is not in buffer pool and apply when they get in buffer pool or running purge operation.

- Unlike clustered index(main index I think), the write on secondary index is very likely not consequective on disk, which leads to random disk access and consumes long time.
  Wait for page to be in buffer pool or do such write together with purge could increase the disk write efficiency.
https://dev.mysql.com/doc/refman/5.7/en/innodb-change-buffer.html

Adaptive hash index
- Hash index is used automically by innodb to achieve better performance. It is helpful for small tables which is likely to fit in memory. In this way the search for item with key is fast.
- There is latch on hashindex so it might be a bottle neck if you have heavy load for  multiple concurrent joins (???) and like or % maching (this makes sense) 
- you can turn it off if needed
- after 56, we have parittion and multiple latch on each parititon for the whole hash index instead of one
https://dev.mysql.com/doc/refman/5.7/en/innodb-adaptive-hash.html


Log buffer -> for redo log
- Buffer to store logs in memory, default to 16MB. It can avoids writing redo log to disk before commit if query is not too big and not fill the buffer. The buffer will periodically
  flush to disk on necessary. Increase the size if you have large write
https://dev.mysql.com/doc/refman/5.7/en/innodb-redo-log-buffer.html


===================================
INNODB DISK STRUCTURE
===================================

- How Secondary Indexes Relate to the Clustered Index
All indexes other than the clustered index are known as secondary indexes. In InnoDB, each record in a secondary index contains the primary key columns for the row, as well as the columns specified for the secondary index. InnoDB uses this primary key value to search for the row in the clustered index.
If the primary key is long, the secondary indexes use more space, so it is advantageous to have a short primary key.

https://dev.mysql.com/doc/refman/5.7/en/innodb-physical-structure.html

--------------------------
Innodb index
spatial index uses R tree, in all other cases, innodb is using btree
Innodb is going to reserve some space in each page (default max is 15/16) for future use (trying to, not guarantee)
If it is below 50% usage, it will do a merge

page size: Supported sizes are 64KB, 32KB, 16KB (default), 8KB, and 4KB. If you define the page size, it can't be changed, and other instance with different page size won't be able to do anything on it for backup etc

- Sorted Index build
This is using a bottom up way of generating the index.
- step one, do a scan with sort buffer, when buffer full, sort write to tmp file
- step two, when all scan done, do a merge sort from files.
- step three, insert into btree
  - bottom up approach: point to right most page of all levels of the tree currently have, insert until we get page full (reaching usage factor), then we move the point to next right page that is allocated, insert.
  - parent link are added when leaf page is full (I assume same for other levels)
  
we still uses innodb_fill_factor to configure the usage of a page when we build it.
Note: the TEXT and BLOB pages which are linked out of the regular leaf pages are not controlled by the usage factor.

https://dev.mysql.com/doc/refman/5.7/en/sorted-index-builds.html

---------------------------
InnoDB Full-Text Index Design
InnoDB FULLTEXT indexes have an inverted index design. Inverted indexes store a list of words, and for each word, a list of documents that the word appears in. To support proximity search, position information for each word is also stored, as a byte offset.

The reverse index is done in a way that is placed in multiple INDEX tables in a partioned fashion. When deletion happens, instead of update all the partitions etc, it is going to add the deletion info in a separate full text deletion table.
When full text related query is executed, it is going to do separate filter and check the delete table to remove results that shouldn't be returned.

The index is store in a way where words is mapping to what document it appears in.

Full text index has an insert buffer to avoid a lot of small writes happens and slow down the DB
https://dev.mysql.com/doc/refman/5.7/en/innodb-fulltext-index.html

---------------------------
Table spaces

The system tablespace is the storage area for the InnoDB data dictionary, the doublewrite buffer, the change buffer, and undo logs. It may also contain table and index data if tables are created in the system tablespace rather than file-per-table or general tablespaces.

The system tablespace can have one or more data files. By default, a single system tablespace data file, named ibdata1, is created in the data directory. The size and number of system tablespace data files is defined by the innodb_data_file_path startup option. For configuration information, see System Tablespace Data File Configuration.

A file-per-table tablespace contains data and indexes for a single InnoDB table, and is stored on the file system in its own data file.

A general tablespace is a shared InnoDB tablespace that is created using CREATE TABLESPACE syntax
- it has slight memory advantage over file per table space as its metadata is kept in memory all the time unlinke the file per table
- it supports everything

Undo tablespaces contain undo logs, which are collections of undo log records that contain information about how to undo the latest change by a transaction to a clustered index record. Undo logs exist within undo log segments, which are contained within rollback segments. The innodb_rollback_segments variable defines the number of rollback segments allocated to each undo tablespace.
Undo logs can be stored in one or more undo tablespaces instead of the system tablespace. This layout differs from the default configuration in which undo logs reside in the system tablespace. The I/O patterns for undo logs make undo tablespaces good candidates for SSD storage, while keeping the system tablespace on hard disk storage.
The number of undo tablespaces used by InnoDB is controlled by the innodb_undo_tablespaces configuration option. This option can only be configured when initializing the MySQL instance. It cannot be changed afterward.
https://dev.mysql.com/doc/refman/5.7/en/innodb-undo-tablespaces.html

temporary tablespace.
Non-compressed, user-created temporary tables and on-disk internal temporary tables are created in a shared temporary tablespace. The innodb_temp_data_file_path configuration option defines the relative path, name, size, and attributes for temporary tablespace data files. If no value is specified for innodb_temp_data_file_path, the default behavior is to create an auto-extending data file named ibtmp1 in the innodb_data_home_dir directory that is slightly larger than 12MB.

This is removed up on restart and recreated. Recreation failure will cause restart failure

------------------------------------
InnoDB Data Dictionary
The InnoDB data dictionary is comprised of internal system tables that contain metadata used to keep track of objects such as tables, indexes, and table columns. The metadata is physically located in the InnoDB system tablespace. For historical reasons, data dictionary metadata overlaps to some degree with information stored in InnoDB table metadata files (.frm files).

-----------------------------------
https://dev.mysql.com/doc/refman/5.7/en/innodb-doublewrite-buffer.html

-----------------------------------
double write buffer

The doublewrite buffer is a storage area where InnoDB writes pages flushed from the buffer pool before writing the pages to their proper positions in the InnoDB data files. If there is an operating system, storage subsystem, or mysqld process crash in the middle of a page write, InnoDB can find a good copy of the page from the doublewrite buffer during crash recovery.

This can provide better recovery strategy, as it only needs a big block write instead of multiple random disk staff


-----------------------------------
Redo log

The redo log is a disk-based data structure used during crash recovery to correct data written by incomplete transactions. During normal operations, the redo log encodes requests to change table data that result from SQL statements or low-level API calls. Modifications that did not finish updating the data files before an unexpected shutdown are replayed automatically during initialization, and before the connections are accepted. For information about the role of the redo log in crash recovery, see Section 14.19.2, “InnoDB Recovery”.

By default, the redo log is physically represented on disk by two files named ib_logfile0 and ib_logfile1. MySQL writes to the redo log files in a circular fashion. Data in the redo log is encoded in terms of records affected; this data is collectively referred to as redo. The passage of data through the redo log is represented by an ever-increasing LSN value.

It helps when the flush data to disk is not done and crash happen in the middle.

-----------------------------------
Undo log

This is used ass crash recovery as well. It is going to be applied to undo log segment and will be used to recover the change if we see some transtractions
that reserve read image for old data. Besides, in recovery it can be used to rollback not committed transtractions.

Innodb has 128 rollback segments and 32 of them are reserved for tmp table name space. Each page can have 16 undo slots, each can map to a transtraction's usage.

Note that one transtraction could need multiple such slot as it can have different operations.

Undo logs are assigned as needed. For example, a transaction that performs INSERT, UPDATE, and DELETE operations on regular and temporary tables requires a full assignment of four undo logs. A transaction that performs only INSERT operations on regular tables requires a single undo log.

A transaction that performs operations on regular tables is assigned undo logs from an assigned system tablespace or undo tablespace rollback segment. A transaction that performs operations on temporary tables is assigned undo logs from an assigned temporary tablespace rollback segment.

If your transtractions use up the undo space, it is going to be stuck somehow. ?????? how to recover? suggested was rerun transtraction
https://dev.mysql.com/doc/refman/5.7/en/innodb-undo-logs.html

=================================================

InnoDB locking

https://dev.mysql.com/doc/refman/5.7/en/innodb-locking.html

--Intention locks
This is a table level lock for multiple lock granlarity. It can be shared or exclusive. The main purpose is to show some trans is locking table/row. (indicate which type of lock (shared or exclusive) a transaction requires later for a row in a table. )
An intention lock will be granted to next trans traction if its need is not conflict with what the current intention lock is for the table. This lock is table level.

--Record locks:
Locking on an index record. Innodb has hidden index by default to help on this lock. If you do select * from xxx where a=1 for update; it will lock records with a=1 in xxx table with this.

--Gap locks:
Locking for gaps, select * from xxx where c BETWEEN 10 and 20 for update; will lock gaps and prevent insert for c in range 10 to 20;

This lock can be hold with conflict, meaning it is just prevent writing, it is not exclusive, i.e. two trans can both block write from 10 to 20.


--Next key locks
record lock + gap lock for gap before the record. This is used in repeatable read which is default to avoid phantom rows.

Insert Intention Locks
An insert intention lock is a type of gap lock set by INSERT operations prior to row insertion. This lock signals the intent to insert in such a way that multiple transactions inserting into the same index gap need not wait for each other if they are not inserting at the same position within the gap

AUTO-INC Locks
An AUTO-INC lock is a special table-level lock taken by transactions inserting into tables with AUTO_INCREMENT columns. In the simplest case, if one transaction is inserting values into the table, any other transactions must wait to do their own inserts into that table, so that rows inserted by the first transaction receive consecutive primary key values.


Predicate Locks for Spatial Indexes
InnoDB supports SPATIAL indexing of columns containing spatial columns (see Section 11.4.8, “Optimizing Spatial Analysis”).
To handle locking for operations involving SPATIAL indexes, next-key locking does not work well to support REPEATABLE READ or SERIALIZABLE transaction isolation levels. There is no absolute ordering concept in multidimensional data, so it is not clear which is the “next” key.

-------------------------------------------------
Transtraction isolation level

https://dev.mysql.com/doc/refman/5.7/en/innodb-transaction-model.html
------------------
Repeatable read:
All following reads are using the snapshot created for the first read.
// needs to confirm whether we support read after write for same trans: yes we have read after write guarantee in same trasntraction for consistent read (which includes staff with snapshot reads):
. The exception to this rule is that the query sees the changes made by earlier statements within the same transaction. This exception causes the following anomaly: If you update some rows in a table, a SELECT sees the latest version of the updated rows, but it might also see older versions of any rows.
https://dev.mysql.com/doc/refman/5.7/en/innodb-consistent-read.html

We could have write sken or lost write in this case still:
14

Repeatable read isolation level guarantees that each transaction will read from the consistent snapshot of the database. In other words, a row is retrieved twice within the same transaction always has similar values.

Many databases such as Postgres, SQLServer in repeatable read isolation level can detect lost update (a special case of write skew) but others don't. (i.e: InnoDB engine in MySQL)

We're back to write skew phenomena problem. There are situations that most of the database engines cannot detect in the repeatable read isolation. One case is when 2 concurrent transactions modifies 2 different objects and making race conditions.

I take an example from the book Designing Data-Intensive Application. Here is the scenario:

You are writing an application for doctors to manage their on-call shifts at a hospital. The hospital usually tries to have several doctors on call at any one time, but it absolutely must have at least one doctor on call. Doctors can give up their shifts (e.g., if they are sick themselves), provided that at least one colleague remains on call in that shift

The next interesting question is how we can implement this under databases. Here is pseudocode SQL code:

BEGIN TRANSACTION;
    SELECT * FROM doctors
        WHERE on_call = true
        AND shift_id = 1234;
    if (current_on_call >= 2) {
        UPDATE doctors
        SET on_call = false WHERE name = 'Alice' AND shift_id = 1234;
    }
COMMIT;  

For locking reads (SELECT with FOR UPDATE or LOCK IN SHARE MODE), UPDATE, and DELETE statements, locking depends on whether the statement uses a unique index with a unique search condition, or a range-type search condition.

For a unique index with a unique search condition, InnoDB locks only the index record found, not the gap before it.

For other search conditions, InnoDB locks the index range scanned, using gap locks or next-key locks to block insertions by other sessions into the gaps covered by the range. For information about gap locks and next-key locks, see Section 14.7.1, “InnoDB Locking”.
---------------------

Read committed:
each read is using a new snapshot.
If you do lock read with update in the select query, the lock only locks the row, not the gap. the gap lock is not used here and insert in gaps are allowed.

row level locking still happens for delted, update
-------------------
read uncommitted:
updates from other trasn even not committed will be read, no lock at all in this case.

Note that this read itself is not consistent, it might read data from multiple versions from mutiple time within the trans instead of a snapshot.
-------------------
SERIALIZABLE

This level is like REPEATABLE READ, but InnoDB implicitly converts all plain SELECT statements to SELECT ... LOCK IN SHARE MODE if autocommit is disabled. If autocommit is enabled, the SELECT is its own transaction. It therefore is known to be read only and can be serialized if performed as a consistent (nonlocking) read and need not block for other transactions. (To force a plain SELECT to block if other transactions have modified the selected rows, disable autocommit.)


==================================
Consistent read info for mysql innodb:
this is used for the snapshot read related isolation levels above. Note this is a name of mysql implementation, not an isolation level.

Consistent read does not work over DROP TABLE(table destroied) or ALTER TABLE(copy is done in this case).


-----------------------
Locking reads:
SELECT ... LOCK IN SHARE MODE
SELECT ... FOR UPDATE
https://dev.mysql.com/doc/refman/5.7/en/innodb-locking-reads.html


----------------------
Phantom row:

The so-called phantom problem occurs within a transaction when the same query produces different sets of rows at different times. For example, if a SELECT is executed twice, but returns a row the second time that was not returned the first time, the row is a “phantom” row.
https://dev.mysql.com/doc/refman/5.7/en/innodb-next-key-locking.html

In my opinion, for same transtraction with repeatable read isolation level, this phantom read would never happen in any case as it is always reading on 
same snapshot whent the trans starts. For update etc, next-key lock is on by default so no one would insert anything in the middle. Even they do the next read won't see it as the snapshot is taken before.
But this do impacts the read committed iolation level as the next-key lock is gone and the snapshot is created on every read action.

--------------------
Dead lock:
Innodb lock upgrade has sequence, if client A request getting X lock on record before B. regardless if B have a S lock or not, A must get the X lock before B get it.
This could lead to dead lock and mysql will throw error and abort the transtraction that tries to create a dead lock.


