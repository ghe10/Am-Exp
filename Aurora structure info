Lecture for gv and stirage basic

Grovereeee provides storage services.
      RDS cp deploy oscar engine in ec2 instances.
      End user: has their own vpc, connect to rds cp (random ec2) at some port.
      CSD: client side driver, allowing rds cp to talk to storage.
      
RDS is storage's customer

Storage has its own vpc. You can't jump through vpc boundary to directly access storage without csd.
Operations related to sto is in background.

====================================================================================================

Aurora : mysql, manf : psql

====================================================================================================

volume: think like a file. old mysql use files, we use volume instead.

|---------------------|
|---------------------|

	|--->| 16 kB pages.

We allows rds cp to access any page and get any number of pages in sequence. => read Dage, write log.

Volumes are separate into protection groups (pg).

Default volume is 160 GB, each PG will has 10 GB data (fixed). (Now we might support less than 160 GB for smaller db)

RDS db instance <==> VOLUME, it is a one to one mapping. 
When we need more data space, we will add new pg to it.

=====================================================================================================

We have i2.8xlarge ec2 instances. has 8 ssds, 800GB each, raw block

these are called sn-demon

we have more than 1000 such nodes in a cluster (across multiple availiability zones).

each sn-demon will manage storage on its machine.

Each pg contains 6 segments on 6 different machines (replication), they are separated amoung three azs. 2 copies in each az.
Then we can tolerate az failure.

=====================================================================================================

Consistency protocol: Quorum, client side driver (csd) need 4/6 for write and 3/6 to read.

During deployment, no more than one segment of each volume is affected.

deployment domain is an aws concept, ?????????

We stop at 34 mins.

======================================================================================================

2018 2 24
tale? segment: for update, not everything
full segment: for data

more like 30GB data + derta

Three segments are T and three are F

so we can have 3/6 full write segment read. ????? really????

=======================================================================================================

storage node

|--------------------|
|--------------------|

	|---| 16 kb page
	 ** derta 1
	 ** derta 2   these things are usually in memory, then go to persisitent storage in disk
	.....

a segemnt in volume, we got a long list of dert.

dert are issued by csd and executed by sn-demon

Read: read the page and apply a lot of update. The dertas are in T segments?????? dert are in inmemory buffer ???

Dertas are in memroy, when go to disk, it is persistent log.

The sn-demons talk to each other to get log sequence number and see if we miss some update log. LSN: log sequence number


recovery: when sn-demon down and restart, it will talk to other sn-demons to fix holes



CPL: consistence point log...

=======================================================================================================

Grover vpc

we have a number of instances (ec2 instances), metadata in ddb. data backuped in S3

we have dependency on swf to execute some things like create volume and so on in backgroud.

We also have cp instances as following:
We also have volume manager(with ebs for log storage), health monitor, workflow, deployment service??, backup manager, restore manager 

Environment set <-> each ec2 instance is tagged by env set
	eg. : VOLUME_MANAGER_SET -> [VOLLUMEN_MANAGER, ..... (some environements)]
	    So we will have /apollo/envSet/env ???


In ddb, we have many tbs. eg, environment table
	we have instance id, ip, environment, health status......... all the related metadata

Next we go through services.
=======================================================================================================
Volumn manager:
    APIS:
	create vol *** most important
	delete vol
	expand volulme
	create net....
	describe volume, pg.....

it will create colume in volumn table in ddb, do workflow reservation (also update ddb). Here we just update ddb!!!!!! SWF will do real work!!!

SWF workflow will watch for changes and read from ddb and really create pgs, segment (16 pg for 160GB default volume, 96 segments 16 * 6) in different zas in ec2 instances

SWF also incharge of delete volumn.

*****
Volumn manager also create anetwork interface to communicate to rds cp (csd actually)


















